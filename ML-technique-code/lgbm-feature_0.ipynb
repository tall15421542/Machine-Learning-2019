{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.load('../X_train.npz')\n",
    "x_train = train_X['arr_0']\n",
    "\n",
    "test_X = np.load('../X_test.npz')\n",
    "x_pred = test_X['arr_0']\n",
    "\n",
    "\n",
    "train_Y = np.load('../Y_train.npz')\n",
    "y_train = (train_Y['arr_0'])\n",
    "\n",
    "pen_rate = y_train[:,0]\n",
    "\n",
    "feature_size = 2000\n",
    "\n",
    "rf_index = []\n",
    "with open(\"rf_index.csv\") as f:\n",
    "    for line in f:\n",
    "        rf_index.append(int(line.strip()))\n",
    "rf_index = np.array(rf_index)\n",
    "\n",
    "x_train = x_train[:,rf_index[:feature_size]]\n",
    "\n",
    "\n",
    "x_pred = x_pred[:,rf_index[:feature_size]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, pen_rate, test_size=0.05, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(x_train,y_train)\n",
    "lgb_eval = lgb.Dataset(x_test,y_test,reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "        'num_leaves': 150,\n",
    "        'max_bin': 255,\n",
    "        'min_data_in_leaf': 1,\n",
    "        'learning_rate': 0.02,\n",
    "        'bagging_fraction': 1.0, \n",
    "        'bagging_freq': 5, \n",
    "        'feature_fraction': 1.0,\n",
    "        'min_gain_to_split': 0.65,\n",
    "        'max_depth': 10,\n",
    "        'save_binary': True,\n",
    "        'seed': 1337,\n",
    "        'feature_fraction_seed': 1337,\n",
    "        'bagging_seed': 1337,\n",
    "        'data_random_seed': 1337,\n",
    "        'objective': 'regression_l1',\n",
    "        'boosting_type': 'gbdt',\n",
    "        #'boosting_type': 'dart',\n",
    "\n",
    "        'verbose': 0,\n",
    "        'metric': {'l1'},\n",
    "        'device': 'gpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's l1: 0.247019\tvalid_1's l1: 0.251022\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's l1: 0.243146\tvalid_1's l1: 0.247449\n",
      "[3]\ttraining's l1: 0.239402\tvalid_1's l1: 0.243894\n",
      "[4]\ttraining's l1: 0.235762\tvalid_1's l1: 0.240389\n",
      "[5]\ttraining's l1: 0.23218\tvalid_1's l1: 0.237001\n",
      "[6]\ttraining's l1: 0.228718\tvalid_1's l1: 0.233676\n",
      "[7]\ttraining's l1: 0.225324\tvalid_1's l1: 0.230473\n",
      "[8]\ttraining's l1: 0.221996\tvalid_1's l1: 0.22733\n",
      "[9]\ttraining's l1: 0.218743\tvalid_1's l1: 0.224284\n",
      "[10]\ttraining's l1: 0.215571\tvalid_1's l1: 0.221275\n",
      "[11]\ttraining's l1: 0.212481\tvalid_1's l1: 0.218345\n",
      "[12]\ttraining's l1: 0.209493\tvalid_1's l1: 0.215493\n",
      "[13]\ttraining's l1: 0.206537\tvalid_1's l1: 0.212668\n",
      "[14]\ttraining's l1: 0.203699\tvalid_1's l1: 0.209986\n",
      "[15]\ttraining's l1: 0.200898\tvalid_1's l1: 0.207386\n",
      "[16]\ttraining's l1: 0.198158\tvalid_1's l1: 0.20478\n",
      "[17]\ttraining's l1: 0.195498\tvalid_1's l1: 0.202284\n",
      "[18]\ttraining's l1: 0.192894\tvalid_1's l1: 0.199866\n",
      "[19]\ttraining's l1: 0.190356\tvalid_1's l1: 0.1975\n",
      "[20]\ttraining's l1: 0.187885\tvalid_1's l1: 0.19518\n",
      "[21]\ttraining's l1: 0.185477\tvalid_1's l1: 0.192932\n",
      "[22]\ttraining's l1: 0.183119\tvalid_1's l1: 0.19072\n",
      "[23]\ttraining's l1: 0.180801\tvalid_1's l1: 0.188539\n",
      "[24]\ttraining's l1: 0.178531\tvalid_1's l1: 0.186414\n",
      "[25]\ttraining's l1: 0.17633\tvalid_1's l1: 0.184334\n",
      "[26]\ttraining's l1: 0.174182\tvalid_1's l1: 0.182364\n",
      "[27]\ttraining's l1: 0.172058\tvalid_1's l1: 0.180364\n",
      "[28]\ttraining's l1: 0.170043\tvalid_1's l1: 0.178484\n",
      "[29]\ttraining's l1: 0.168052\tvalid_1's l1: 0.176614\n",
      "[30]\ttraining's l1: 0.166086\tvalid_1's l1: 0.174728\n",
      "[31]\ttraining's l1: 0.164185\tvalid_1's l1: 0.172972\n",
      "[32]\ttraining's l1: 0.162345\tvalid_1's l1: 0.171262\n",
      "[33]\ttraining's l1: 0.160542\tvalid_1's l1: 0.169517\n",
      "[34]\ttraining's l1: 0.15879\tvalid_1's l1: 0.167903\n",
      "[35]\ttraining's l1: 0.157063\tvalid_1's l1: 0.166316\n",
      "[36]\ttraining's l1: 0.155398\tvalid_1's l1: 0.164762\n",
      "[37]\ttraining's l1: 0.153759\tvalid_1's l1: 0.163229\n",
      "[38]\ttraining's l1: 0.152153\tvalid_1's l1: 0.161735\n",
      "[39]\ttraining's l1: 0.150558\tvalid_1's l1: 0.160256\n",
      "[40]\ttraining's l1: 0.149028\tvalid_1's l1: 0.158795\n",
      "[41]\ttraining's l1: 0.147523\tvalid_1's l1: 0.157392\n",
      "[42]\ttraining's l1: 0.146077\tvalid_1's l1: 0.156063\n",
      "[43]\ttraining's l1: 0.144647\tvalid_1's l1: 0.154738\n",
      "[44]\ttraining's l1: 0.14323\tvalid_1's l1: 0.15346\n",
      "[45]\ttraining's l1: 0.14187\tvalid_1's l1: 0.152181\n",
      "[46]\ttraining's l1: 0.140524\tvalid_1's l1: 0.150953\n",
      "[47]\ttraining's l1: 0.139214\tvalid_1's l1: 0.149704\n",
      "[48]\ttraining's l1: 0.137934\tvalid_1's l1: 0.148547\n",
      "[49]\ttraining's l1: 0.13668\tvalid_1's l1: 0.147419\n",
      "[50]\ttraining's l1: 0.135473\tvalid_1's l1: 0.146321\n",
      "[51]\ttraining's l1: 0.134317\tvalid_1's l1: 0.145281\n",
      "[52]\ttraining's l1: 0.133148\tvalid_1's l1: 0.144211\n",
      "[53]\ttraining's l1: 0.132033\tvalid_1's l1: 0.143171\n",
      "[54]\ttraining's l1: 0.130923\tvalid_1's l1: 0.142172\n",
      "[55]\ttraining's l1: 0.129818\tvalid_1's l1: 0.141186\n",
      "[56]\ttraining's l1: 0.128744\tvalid_1's l1: 0.140228\n",
      "[57]\ttraining's l1: 0.127718\tvalid_1's l1: 0.139284\n",
      "[58]\ttraining's l1: 0.126722\tvalid_1's l1: 0.138372\n",
      "[59]\ttraining's l1: 0.125736\tvalid_1's l1: 0.137486\n",
      "[60]\ttraining's l1: 0.124766\tvalid_1's l1: 0.136598\n",
      "[61]\ttraining's l1: 0.12383\tvalid_1's l1: 0.135742\n",
      "[62]\ttraining's l1: 0.122915\tvalid_1's l1: 0.134921\n",
      "[63]\ttraining's l1: 0.122028\tvalid_1's l1: 0.134135\n",
      "[64]\ttraining's l1: 0.121126\tvalid_1's l1: 0.133327\n",
      "[65]\ttraining's l1: 0.120256\tvalid_1's l1: 0.132532\n",
      "[66]\ttraining's l1: 0.119417\tvalid_1's l1: 0.131785\n",
      "[67]\ttraining's l1: 0.118595\tvalid_1's l1: 0.131026\n",
      "[68]\ttraining's l1: 0.117781\tvalid_1's l1: 0.130283\n",
      "[69]\ttraining's l1: 0.117012\tvalid_1's l1: 0.129601\n",
      "[70]\ttraining's l1: 0.116227\tvalid_1's l1: 0.1289\n",
      "[71]\ttraining's l1: 0.115471\tvalid_1's l1: 0.128238\n",
      "[72]\ttraining's l1: 0.114734\tvalid_1's l1: 0.127567\n",
      "[73]\ttraining's l1: 0.114007\tvalid_1's l1: 0.126908\n",
      "[74]\ttraining's l1: 0.113292\tvalid_1's l1: 0.12629\n",
      "[75]\ttraining's l1: 0.1126\tvalid_1's l1: 0.125669\n",
      "[76]\ttraining's l1: 0.111932\tvalid_1's l1: 0.125073\n",
      "[77]\ttraining's l1: 0.111275\tvalid_1's l1: 0.1245\n",
      "[78]\ttraining's l1: 0.110627\tvalid_1's l1: 0.123933\n",
      "[79]\ttraining's l1: 0.110005\tvalid_1's l1: 0.123393\n",
      "[80]\ttraining's l1: 0.109381\tvalid_1's l1: 0.1228\n",
      "[81]\ttraining's l1: 0.108778\tvalid_1's l1: 0.122255\n",
      "[82]\ttraining's l1: 0.10818\tvalid_1's l1: 0.121716\n",
      "[83]\ttraining's l1: 0.107607\tvalid_1's l1: 0.121224\n",
      "[84]\ttraining's l1: 0.107021\tvalid_1's l1: 0.12069\n",
      "[85]\ttraining's l1: 0.106456\tvalid_1's l1: 0.120213\n",
      "[86]\ttraining's l1: 0.105888\tvalid_1's l1: 0.119705\n",
      "[87]\ttraining's l1: 0.105342\tvalid_1's l1: 0.11924\n",
      "[88]\ttraining's l1: 0.104813\tvalid_1's l1: 0.118782\n",
      "[89]\ttraining's l1: 0.104292\tvalid_1's l1: 0.118305\n",
      "[90]\ttraining's l1: 0.103783\tvalid_1's l1: 0.117874\n",
      "[91]\ttraining's l1: 0.103291\tvalid_1's l1: 0.117464\n",
      "[92]\ttraining's l1: 0.102802\tvalid_1's l1: 0.117015\n",
      "[93]\ttraining's l1: 0.102338\tvalid_1's l1: 0.116608\n",
      "[94]\ttraining's l1: 0.101857\tvalid_1's l1: 0.116174\n",
      "[95]\ttraining's l1: 0.101396\tvalid_1's l1: 0.115768\n",
      "[96]\ttraining's l1: 0.100941\tvalid_1's l1: 0.115354\n",
      "[97]\ttraining's l1: 0.100496\tvalid_1's l1: 0.114994\n",
      "[98]\ttraining's l1: 0.10007\tvalid_1's l1: 0.114622\n",
      "[99]\ttraining's l1: 0.0996497\tvalid_1's l1: 0.114263\n",
      "[100]\ttraining's l1: 0.099241\tvalid_1's l1: 0.113903\n",
      "[101]\ttraining's l1: 0.0988264\tvalid_1's l1: 0.11355\n",
      "[102]\ttraining's l1: 0.0984308\tvalid_1's l1: 0.113198\n",
      "[103]\ttraining's l1: 0.0980322\tvalid_1's l1: 0.112863\n",
      "[104]\ttraining's l1: 0.0976478\tvalid_1's l1: 0.112533\n",
      "[105]\ttraining's l1: 0.0972787\tvalid_1's l1: 0.112216\n",
      "[106]\ttraining's l1: 0.0969124\tvalid_1's l1: 0.1119\n",
      "[107]\ttraining's l1: 0.0965694\tvalid_1's l1: 0.111615\n",
      "[108]\ttraining's l1: 0.0962201\tvalid_1's l1: 0.111325\n",
      "[109]\ttraining's l1: 0.0958803\tvalid_1's l1: 0.111047\n",
      "[110]\ttraining's l1: 0.0955362\tvalid_1's l1: 0.110757\n",
      "[111]\ttraining's l1: 0.0952115\tvalid_1's l1: 0.110493\n",
      "[112]\ttraining's l1: 0.094895\tvalid_1's l1: 0.110219\n",
      "[113]\ttraining's l1: 0.0945844\tvalid_1's l1: 0.109988\n",
      "[114]\ttraining's l1: 0.0942807\tvalid_1's l1: 0.109742\n",
      "[115]\ttraining's l1: 0.0939785\tvalid_1's l1: 0.109506\n",
      "[116]\ttraining's l1: 0.0936887\tvalid_1's l1: 0.109293\n",
      "[117]\ttraining's l1: 0.0933996\tvalid_1's l1: 0.109041\n",
      "[118]\ttraining's l1: 0.0931168\tvalid_1's l1: 0.108818\n",
      "[119]\ttraining's l1: 0.0928401\tvalid_1's l1: 0.108572\n",
      "[120]\ttraining's l1: 0.0925627\tvalid_1's l1: 0.10835\n",
      "[121]\ttraining's l1: 0.0922815\tvalid_1's l1: 0.108121\n",
      "[122]\ttraining's l1: 0.0920171\tvalid_1's l1: 0.107922\n",
      "[123]\ttraining's l1: 0.0917564\tvalid_1's l1: 0.107716\n",
      "[124]\ttraining's l1: 0.0914913\tvalid_1's l1: 0.107515\n",
      "[125]\ttraining's l1: 0.09123\tvalid_1's l1: 0.107325\n",
      "[126]\ttraining's l1: 0.0909823\tvalid_1's l1: 0.107116\n",
      "[127]\ttraining's l1: 0.0907361\tvalid_1's l1: 0.106934\n",
      "[128]\ttraining's l1: 0.0904939\tvalid_1's l1: 0.10675\n",
      "[129]\ttraining's l1: 0.0902673\tvalid_1's l1: 0.106571\n",
      "[130]\ttraining's l1: 0.0900372\tvalid_1's l1: 0.106384\n",
      "[131]\ttraining's l1: 0.0898095\tvalid_1's l1: 0.106201\n",
      "[132]\ttraining's l1: 0.0895825\tvalid_1's l1: 0.106033\n",
      "[133]\ttraining's l1: 0.0893631\tvalid_1's l1: 0.10587\n",
      "[134]\ttraining's l1: 0.0891542\tvalid_1's l1: 0.105726\n",
      "[135]\ttraining's l1: 0.0889258\tvalid_1's l1: 0.10556\n",
      "[136]\ttraining's l1: 0.0887235\tvalid_1's l1: 0.105408\n",
      "[137]\ttraining's l1: 0.0885219\tvalid_1's l1: 0.105273\n",
      "[138]\ttraining's l1: 0.0883183\tvalid_1's l1: 0.105116\n",
      "[139]\ttraining's l1: 0.0881195\tvalid_1's l1: 0.104961\n",
      "[140]\ttraining's l1: 0.0879279\tvalid_1's l1: 0.104826\n",
      "[141]\ttraining's l1: 0.0877388\tvalid_1's l1: 0.104686\n",
      "[142]\ttraining's l1: 0.0875437\tvalid_1's l1: 0.104567\n",
      "[143]\ttraining's l1: 0.0873538\tvalid_1's l1: 0.104428\n",
      "[144]\ttraining's l1: 0.0871774\tvalid_1's l1: 0.104296\n",
      "[145]\ttraining's l1: 0.0870052\tvalid_1's l1: 0.104152\n",
      "[146]\ttraining's l1: 0.0868248\tvalid_1's l1: 0.104014\n",
      "[147]\ttraining's l1: 0.0866513\tvalid_1's l1: 0.103906\n",
      "[148]\ttraining's l1: 0.0864853\tvalid_1's l1: 0.1038\n",
      "[149]\ttraining's l1: 0.0863167\tvalid_1's l1: 0.103667\n",
      "[150]\ttraining's l1: 0.0861641\tvalid_1's l1: 0.103579\n",
      "[151]\ttraining's l1: 0.0860155\tvalid_1's l1: 0.103477\n",
      "[152]\ttraining's l1: 0.0858619\tvalid_1's l1: 0.103367\n",
      "[153]\ttraining's l1: 0.0857082\tvalid_1's l1: 0.103251\n",
      "[154]\ttraining's l1: 0.0855644\tvalid_1's l1: 0.103165\n",
      "[155]\ttraining's l1: 0.0854173\tvalid_1's l1: 0.103076\n",
      "[156]\ttraining's l1: 0.0852729\tvalid_1's l1: 0.102986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[157]\ttraining's l1: 0.0851331\tvalid_1's l1: 0.1029\n",
      "[158]\ttraining's l1: 0.0850003\tvalid_1's l1: 0.102813\n",
      "[159]\ttraining's l1: 0.0848593\tvalid_1's l1: 0.102741\n",
      "[160]\ttraining's l1: 0.0847293\tvalid_1's l1: 0.102655\n",
      "[161]\ttraining's l1: 0.0845915\tvalid_1's l1: 0.102588\n",
      "[162]\ttraining's l1: 0.0844527\tvalid_1's l1: 0.10252\n",
      "[163]\ttraining's l1: 0.0843167\tvalid_1's l1: 0.102446\n",
      "[164]\ttraining's l1: 0.08418\tvalid_1's l1: 0.102356\n",
      "[165]\ttraining's l1: 0.0840495\tvalid_1's l1: 0.102281\n",
      "[166]\ttraining's l1: 0.0839251\tvalid_1's l1: 0.102209\n",
      "[167]\ttraining's l1: 0.083803\tvalid_1's l1: 0.102149\n",
      "[168]\ttraining's l1: 0.0836954\tvalid_1's l1: 0.102085\n",
      "[169]\ttraining's l1: 0.0835723\tvalid_1's l1: 0.102004\n",
      "[170]\ttraining's l1: 0.08344\tvalid_1's l1: 0.101913\n",
      "[171]\ttraining's l1: 0.0833196\tvalid_1's l1: 0.101854\n",
      "[172]\ttraining's l1: 0.0832037\tvalid_1's l1: 0.101779\n",
      "[173]\ttraining's l1: 0.083073\tvalid_1's l1: 0.101711\n",
      "[174]\ttraining's l1: 0.0829633\tvalid_1's l1: 0.101643\n",
      "[175]\ttraining's l1: 0.0828561\tvalid_1's l1: 0.101585\n",
      "[176]\ttraining's l1: 0.0827432\tvalid_1's l1: 0.101534\n",
      "[177]\ttraining's l1: 0.0826211\tvalid_1's l1: 0.101466\n",
      "[178]\ttraining's l1: 0.0825144\tvalid_1's l1: 0.101412\n",
      "[179]\ttraining's l1: 0.0824166\tvalid_1's l1: 0.101352\n",
      "[180]\ttraining's l1: 0.0823152\tvalid_1's l1: 0.101303\n",
      "[181]\ttraining's l1: 0.0822135\tvalid_1's l1: 0.101251\n",
      "[182]\ttraining's l1: 0.0821062\tvalid_1's l1: 0.101198\n",
      "[183]\ttraining's l1: 0.0820131\tvalid_1's l1: 0.101147\n",
      "[184]\ttraining's l1: 0.0819019\tvalid_1's l1: 0.101084\n",
      "[185]\ttraining's l1: 0.0818013\tvalid_1's l1: 0.101046\n",
      "[186]\ttraining's l1: 0.0817104\tvalid_1's l1: 0.100994\n",
      "[187]\ttraining's l1: 0.0816203\tvalid_1's l1: 0.100944\n",
      "[188]\ttraining's l1: 0.081536\tvalid_1's l1: 0.100893\n",
      "[189]\ttraining's l1: 0.0814414\tvalid_1's l1: 0.100867\n",
      "[190]\ttraining's l1: 0.0813604\tvalid_1's l1: 0.100826\n",
      "[191]\ttraining's l1: 0.0812686\tvalid_1's l1: 0.100785\n",
      "[192]\ttraining's l1: 0.0811904\tvalid_1's l1: 0.100737\n",
      "[193]\ttraining's l1: 0.0811079\tvalid_1's l1: 0.100701\n",
      "[194]\ttraining's l1: 0.0810308\tvalid_1's l1: 0.100655\n",
      "[195]\ttraining's l1: 0.0809417\tvalid_1's l1: 0.10062\n",
      "[196]\ttraining's l1: 0.080862\tvalid_1's l1: 0.100593\n",
      "[197]\ttraining's l1: 0.0807846\tvalid_1's l1: 0.100566\n",
      "[198]\ttraining's l1: 0.080706\tvalid_1's l1: 0.100545\n",
      "[199]\ttraining's l1: 0.0806402\tvalid_1's l1: 0.100511\n",
      "[200]\ttraining's l1: 0.080562\tvalid_1's l1: 0.100476\n",
      "[201]\ttraining's l1: 0.0804846\tvalid_1's l1: 0.100452\n",
      "[202]\ttraining's l1: 0.0804307\tvalid_1's l1: 0.100425\n",
      "[203]\ttraining's l1: 0.0803688\tvalid_1's l1: 0.1004\n",
      "[204]\ttraining's l1: 0.080309\tvalid_1's l1: 0.100374\n",
      "[205]\ttraining's l1: 0.0802362\tvalid_1's l1: 0.100341\n",
      "[206]\ttraining's l1: 0.0801729\tvalid_1's l1: 0.100318\n",
      "[207]\ttraining's l1: 0.0801028\tvalid_1's l1: 0.100292\n",
      "[208]\ttraining's l1: 0.0800431\tvalid_1's l1: 0.100271\n",
      "[209]\ttraining's l1: 0.0799829\tvalid_1's l1: 0.100256\n",
      "[210]\ttraining's l1: 0.0799271\tvalid_1's l1: 0.100246\n",
      "[211]\ttraining's l1: 0.0798655\tvalid_1's l1: 0.100212\n",
      "[212]\ttraining's l1: 0.0797855\tvalid_1's l1: 0.10017\n",
      "[213]\ttraining's l1: 0.0797274\tvalid_1's l1: 0.100139\n",
      "[214]\ttraining's l1: 0.079665\tvalid_1's l1: 0.100114\n",
      "[215]\ttraining's l1: 0.079603\tvalid_1's l1: 0.100082\n",
      "[216]\ttraining's l1: 0.0795399\tvalid_1's l1: 0.10005\n",
      "[217]\ttraining's l1: 0.0794579\tvalid_1's l1: 0.100018\n",
      "[218]\ttraining's l1: 0.0794112\tvalid_1's l1: 0.0999807\n",
      "[219]\ttraining's l1: 0.0793652\tvalid_1's l1: 0.0999576\n",
      "[220]\ttraining's l1: 0.0793055\tvalid_1's l1: 0.0999354\n",
      "[221]\ttraining's l1: 0.0792399\tvalid_1's l1: 0.0999014\n",
      "[222]\ttraining's l1: 0.0791866\tvalid_1's l1: 0.0998951\n",
      "[223]\ttraining's l1: 0.0791317\tvalid_1's l1: 0.099871\n",
      "[224]\ttraining's l1: 0.079063\tvalid_1's l1: 0.0998536\n",
      "[225]\ttraining's l1: 0.0790063\tvalid_1's l1: 0.0998239\n",
      "[226]\ttraining's l1: 0.0789615\tvalid_1's l1: 0.099802\n",
      "[227]\ttraining's l1: 0.0789197\tvalid_1's l1: 0.0997864\n",
      "[228]\ttraining's l1: 0.0788555\tvalid_1's l1: 0.0997542\n",
      "[229]\ttraining's l1: 0.0788171\tvalid_1's l1: 0.099744\n",
      "[230]\ttraining's l1: 0.0787443\tvalid_1's l1: 0.0997298\n",
      "[231]\ttraining's l1: 0.0786968\tvalid_1's l1: 0.0997029\n",
      "[232]\ttraining's l1: 0.0786504\tvalid_1's l1: 0.0996732\n",
      "[233]\ttraining's l1: 0.0785932\tvalid_1's l1: 0.0996594\n",
      "[234]\ttraining's l1: 0.0785393\tvalid_1's l1: 0.0996352\n",
      "[235]\ttraining's l1: 0.0784893\tvalid_1's l1: 0.0996148\n",
      "[236]\ttraining's l1: 0.0784193\tvalid_1's l1: 0.0995914\n",
      "[237]\ttraining's l1: 0.0783821\tvalid_1's l1: 0.0995754\n",
      "[238]\ttraining's l1: 0.0783337\tvalid_1's l1: 0.0995558\n",
      "[239]\ttraining's l1: 0.0782797\tvalid_1's l1: 0.0995358\n",
      "[240]\ttraining's l1: 0.0782404\tvalid_1's l1: 0.0995218\n",
      "[241]\ttraining's l1: 0.0781973\tvalid_1's l1: 0.0994979\n",
      "[242]\ttraining's l1: 0.0781423\tvalid_1's l1: 0.0994739\n",
      "[243]\ttraining's l1: 0.0780834\tvalid_1's l1: 0.0994613\n",
      "[244]\ttraining's l1: 0.0780217\tvalid_1's l1: 0.0994435\n",
      "[245]\ttraining's l1: 0.0779788\tvalid_1's l1: 0.0994291\n",
      "[246]\ttraining's l1: 0.0779215\tvalid_1's l1: 0.0994127\n",
      "[247]\ttraining's l1: 0.0778668\tvalid_1's l1: 0.0994013\n",
      "[248]\ttraining's l1: 0.0778149\tvalid_1's l1: 0.0993663\n",
      "[249]\ttraining's l1: 0.0777812\tvalid_1's l1: 0.0993563\n",
      "[250]\ttraining's l1: 0.0777345\tvalid_1's l1: 0.0993307\n",
      "[251]\ttraining's l1: 0.0776888\tvalid_1's l1: 0.0993121\n",
      "[252]\ttraining's l1: 0.0776306\tvalid_1's l1: 0.0992999\n",
      "[253]\ttraining's l1: 0.0775848\tvalid_1's l1: 0.0992769\n",
      "[254]\ttraining's l1: 0.0775473\tvalid_1's l1: 0.0992691\n",
      "[255]\ttraining's l1: 0.0775013\tvalid_1's l1: 0.099245\n",
      "[256]\ttraining's l1: 0.0774534\tvalid_1's l1: 0.0992465\n",
      "[257]\ttraining's l1: 0.077406\tvalid_1's l1: 0.0992263\n",
      "[258]\ttraining's l1: 0.0773559\tvalid_1's l1: 0.0992156\n",
      "[259]\ttraining's l1: 0.0773248\tvalid_1's l1: 0.0992108\n",
      "[260]\ttraining's l1: 0.0772918\tvalid_1's l1: 0.0991947\n",
      "[261]\ttraining's l1: 0.0772425\tvalid_1's l1: 0.0991884\n",
      "[262]\ttraining's l1: 0.0771945\tvalid_1's l1: 0.0991736\n",
      "[263]\ttraining's l1: 0.07714\tvalid_1's l1: 0.0991627\n",
      "[264]\ttraining's l1: 0.0771046\tvalid_1's l1: 0.0991564\n",
      "[265]\ttraining's l1: 0.0770765\tvalid_1's l1: 0.0991481\n",
      "[266]\ttraining's l1: 0.0770448\tvalid_1's l1: 0.0991298\n",
      "[267]\ttraining's l1: 0.0770065\tvalid_1's l1: 0.0991132\n",
      "[268]\ttraining's l1: 0.076952\tvalid_1's l1: 0.0991014\n",
      "[269]\ttraining's l1: 0.0769141\tvalid_1's l1: 0.0990886\n",
      "[270]\ttraining's l1: 0.0768758\tvalid_1's l1: 0.0990837\n",
      "[271]\ttraining's l1: 0.0768337\tvalid_1's l1: 0.0990699\n",
      "[272]\ttraining's l1: 0.0767973\tvalid_1's l1: 0.0990569\n",
      "[273]\ttraining's l1: 0.076748\tvalid_1's l1: 0.0990411\n",
      "[274]\ttraining's l1: 0.0767178\tvalid_1's l1: 0.099031\n",
      "[275]\ttraining's l1: 0.0766919\tvalid_1's l1: 0.0990222\n",
      "[276]\ttraining's l1: 0.0766475\tvalid_1's l1: 0.0990026\n",
      "[277]\ttraining's l1: 0.0766082\tvalid_1's l1: 0.0989932\n",
      "[278]\ttraining's l1: 0.0765447\tvalid_1's l1: 0.098978\n",
      "[279]\ttraining's l1: 0.0765126\tvalid_1's l1: 0.0989691\n",
      "[280]\ttraining's l1: 0.0764756\tvalid_1's l1: 0.0989631\n",
      "[281]\ttraining's l1: 0.0764526\tvalid_1's l1: 0.0989566\n",
      "[282]\ttraining's l1: 0.0764189\tvalid_1's l1: 0.0989537\n",
      "[283]\ttraining's l1: 0.0763753\tvalid_1's l1: 0.0989478\n",
      "[284]\ttraining's l1: 0.0763287\tvalid_1's l1: 0.0989366\n",
      "[285]\ttraining's l1: 0.0762929\tvalid_1's l1: 0.0989193\n",
      "[286]\ttraining's l1: 0.076253\tvalid_1's l1: 0.098909\n",
      "[287]\ttraining's l1: 0.0762175\tvalid_1's l1: 0.0988853\n",
      "[288]\ttraining's l1: 0.0761842\tvalid_1's l1: 0.0988715\n",
      "[289]\ttraining's l1: 0.0761469\tvalid_1's l1: 0.098863\n",
      "[290]\ttraining's l1: 0.0761146\tvalid_1's l1: 0.0988583\n",
      "[291]\ttraining's l1: 0.0760684\tvalid_1's l1: 0.0988376\n",
      "[292]\ttraining's l1: 0.0760356\tvalid_1's l1: 0.0988221\n",
      "[293]\ttraining's l1: 0.0760136\tvalid_1's l1: 0.0988177\n",
      "[294]\ttraining's l1: 0.0759632\tvalid_1's l1: 0.0988126\n",
      "[295]\ttraining's l1: 0.0759431\tvalid_1's l1: 0.098808\n",
      "[296]\ttraining's l1: 0.0759291\tvalid_1's l1: 0.0988075\n",
      "[297]\ttraining's l1: 0.0759107\tvalid_1's l1: 0.098803\n",
      "[298]\ttraining's l1: 0.075883\tvalid_1's l1: 0.0987974\n",
      "[299]\ttraining's l1: 0.0758563\tvalid_1's l1: 0.0987901\n",
      "[300]\ttraining's l1: 0.0758245\tvalid_1's l1: 0.0987856\n",
      "[301]\ttraining's l1: 0.0758014\tvalid_1's l1: 0.0987837\n",
      "[302]\ttraining's l1: 0.0757745\tvalid_1's l1: 0.098769\n",
      "[303]\ttraining's l1: 0.0757373\tvalid_1's l1: 0.0987589\n",
      "[304]\ttraining's l1: 0.075688\tvalid_1's l1: 0.0987515\n",
      "[305]\ttraining's l1: 0.0756532\tvalid_1's l1: 0.0987412\n",
      "[306]\ttraining's l1: 0.0756096\tvalid_1's l1: 0.0987326\n",
      "[307]\ttraining's l1: 0.0755553\tvalid_1's l1: 0.0987224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[308]\ttraining's l1: 0.0755094\tvalid_1's l1: 0.0987087\n",
      "[309]\ttraining's l1: 0.075475\tvalid_1's l1: 0.0987091\n",
      "[310]\ttraining's l1: 0.0754354\tvalid_1's l1: 0.0987086\n",
      "[311]\ttraining's l1: 0.0754091\tvalid_1's l1: 0.0987044\n",
      "[312]\ttraining's l1: 0.0753804\tvalid_1's l1: 0.0986959\n",
      "[313]\ttraining's l1: 0.0753397\tvalid_1's l1: 0.0986852\n",
      "[314]\ttraining's l1: 0.0753164\tvalid_1's l1: 0.0986806\n",
      "[315]\ttraining's l1: 0.0752755\tvalid_1's l1: 0.098672\n",
      "[316]\ttraining's l1: 0.0752482\tvalid_1's l1: 0.0986687\n",
      "[317]\ttraining's l1: 0.0752139\tvalid_1's l1: 0.0986542\n",
      "[318]\ttraining's l1: 0.0751903\tvalid_1's l1: 0.09865\n",
      "[319]\ttraining's l1: 0.0751614\tvalid_1's l1: 0.0986469\n",
      "[320]\ttraining's l1: 0.0751227\tvalid_1's l1: 0.0986383\n",
      "[321]\ttraining's l1: 0.0750719\tvalid_1's l1: 0.098626\n",
      "[322]\ttraining's l1: 0.0750447\tvalid_1's l1: 0.0986253\n",
      "[323]\ttraining's l1: 0.0750205\tvalid_1's l1: 0.0986143\n",
      "[324]\ttraining's l1: 0.074985\tvalid_1's l1: 0.0986031\n",
      "[325]\ttraining's l1: 0.0749584\tvalid_1's l1: 0.0985891\n",
      "[326]\ttraining's l1: 0.074927\tvalid_1's l1: 0.0985872\n",
      "[327]\ttraining's l1: 0.0748885\tvalid_1's l1: 0.098572\n",
      "[328]\ttraining's l1: 0.0748533\tvalid_1's l1: 0.0985594\n",
      "[329]\ttraining's l1: 0.0748122\tvalid_1's l1: 0.0985518\n",
      "[330]\ttraining's l1: 0.0747754\tvalid_1's l1: 0.0985476\n",
      "[331]\ttraining's l1: 0.0747327\tvalid_1's l1: 0.0985297\n",
      "[332]\ttraining's l1: 0.0746829\tvalid_1's l1: 0.0985078\n",
      "[333]\ttraining's l1: 0.0746147\tvalid_1's l1: 0.0984805\n",
      "[334]\ttraining's l1: 0.0745763\tvalid_1's l1: 0.098472\n",
      "[335]\ttraining's l1: 0.0745384\tvalid_1's l1: 0.0984585\n",
      "[336]\ttraining's l1: 0.0745025\tvalid_1's l1: 0.0984496\n",
      "[337]\ttraining's l1: 0.0744657\tvalid_1's l1: 0.0984535\n",
      "[338]\ttraining's l1: 0.0744405\tvalid_1's l1: 0.0984443\n",
      "[339]\ttraining's l1: 0.0744065\tvalid_1's l1: 0.0984367\n",
      "[340]\ttraining's l1: 0.0743687\tvalid_1's l1: 0.0984343\n",
      "[341]\ttraining's l1: 0.0743317\tvalid_1's l1: 0.0984299\n",
      "[342]\ttraining's l1: 0.0742991\tvalid_1's l1: 0.0984252\n",
      "[343]\ttraining's l1: 0.0742666\tvalid_1's l1: 0.0984119\n",
      "[344]\ttraining's l1: 0.0742243\tvalid_1's l1: 0.0983963\n",
      "[345]\ttraining's l1: 0.0741951\tvalid_1's l1: 0.0983937\n",
      "[346]\ttraining's l1: 0.0741602\tvalid_1's l1: 0.098382\n",
      "[347]\ttraining's l1: 0.0741285\tvalid_1's l1: 0.0983869\n",
      "[348]\ttraining's l1: 0.0740747\tvalid_1's l1: 0.098366\n",
      "[349]\ttraining's l1: 0.074046\tvalid_1's l1: 0.0983643\n",
      "[350]\ttraining's l1: 0.0740199\tvalid_1's l1: 0.098363\n",
      "[351]\ttraining's l1: 0.073979\tvalid_1's l1: 0.0983527\n",
      "[352]\ttraining's l1: 0.0739443\tvalid_1's l1: 0.0983488\n",
      "[353]\ttraining's l1: 0.0739183\tvalid_1's l1: 0.098353\n",
      "[354]\ttraining's l1: 0.0738922\tvalid_1's l1: 0.0983401\n",
      "[355]\ttraining's l1: 0.0738686\tvalid_1's l1: 0.0983423\n",
      "[356]\ttraining's l1: 0.0738483\tvalid_1's l1: 0.0983354\n",
      "[357]\ttraining's l1: 0.0738202\tvalid_1's l1: 0.0983254\n",
      "[358]\ttraining's l1: 0.0737934\tvalid_1's l1: 0.0983209\n",
      "[359]\ttraining's l1: 0.0737681\tvalid_1's l1: 0.0983226\n",
      "[360]\ttraining's l1: 0.0737386\tvalid_1's l1: 0.0983129\n",
      "[361]\ttraining's l1: 0.0737039\tvalid_1's l1: 0.0983038\n",
      "[362]\ttraining's l1: 0.0736801\tvalid_1's l1: 0.0982926\n",
      "[363]\ttraining's l1: 0.073659\tvalid_1's l1: 0.0982903\n",
      "[364]\ttraining's l1: 0.0736277\tvalid_1's l1: 0.0982907\n",
      "[365]\ttraining's l1: 0.0735852\tvalid_1's l1: 0.0982725\n",
      "[366]\ttraining's l1: 0.0735376\tvalid_1's l1: 0.098256\n",
      "[367]\ttraining's l1: 0.073504\tvalid_1's l1: 0.0982495\n",
      "[368]\ttraining's l1: 0.073483\tvalid_1's l1: 0.0982466\n",
      "[369]\ttraining's l1: 0.0734507\tvalid_1's l1: 0.0982456\n",
      "[370]\ttraining's l1: 0.0734112\tvalid_1's l1: 0.098227\n",
      "[371]\ttraining's l1: 0.0733813\tvalid_1's l1: 0.0982271\n",
      "[372]\ttraining's l1: 0.0733553\tvalid_1's l1: 0.0982211\n",
      "[373]\ttraining's l1: 0.0733277\tvalid_1's l1: 0.0982112\n",
      "[374]\ttraining's l1: 0.0732944\tvalid_1's l1: 0.0982106\n",
      "[375]\ttraining's l1: 0.0732677\tvalid_1's l1: 0.0982007\n",
      "[376]\ttraining's l1: 0.0732391\tvalid_1's l1: 0.0981826\n",
      "[377]\ttraining's l1: 0.0732149\tvalid_1's l1: 0.0981738\n",
      "[378]\ttraining's l1: 0.073192\tvalid_1's l1: 0.0981673\n",
      "[379]\ttraining's l1: 0.0731637\tvalid_1's l1: 0.0981558\n",
      "[380]\ttraining's l1: 0.0731324\tvalid_1's l1: 0.0981513\n",
      "[381]\ttraining's l1: 0.0731043\tvalid_1's l1: 0.0981509\n",
      "[382]\ttraining's l1: 0.0730727\tvalid_1's l1: 0.098142\n",
      "[383]\ttraining's l1: 0.0730389\tvalid_1's l1: 0.0981381\n",
      "[384]\ttraining's l1: 0.0730142\tvalid_1's l1: 0.098141\n",
      "[385]\ttraining's l1: 0.0729877\tvalid_1's l1: 0.0981287\n",
      "[386]\ttraining's l1: 0.0729599\tvalid_1's l1: 0.0981177\n",
      "[387]\ttraining's l1: 0.072927\tvalid_1's l1: 0.0981162\n",
      "[388]\ttraining's l1: 0.0728946\tvalid_1's l1: 0.0981067\n",
      "[389]\ttraining's l1: 0.0728725\tvalid_1's l1: 0.0981044\n",
      "[390]\ttraining's l1: 0.0728449\tvalid_1's l1: 0.0980979\n",
      "[391]\ttraining's l1: 0.0728149\tvalid_1's l1: 0.0980862\n",
      "[392]\ttraining's l1: 0.0727811\tvalid_1's l1: 0.098085\n",
      "[393]\ttraining's l1: 0.0727501\tvalid_1's l1: 0.0980825\n",
      "[394]\ttraining's l1: 0.0727252\tvalid_1's l1: 0.0980775\n",
      "[395]\ttraining's l1: 0.072704\tvalid_1's l1: 0.0980748\n",
      "[396]\ttraining's l1: 0.0726809\tvalid_1's l1: 0.0980677\n",
      "[397]\ttraining's l1: 0.0726566\tvalid_1's l1: 0.0980601\n",
      "[398]\ttraining's l1: 0.0726324\tvalid_1's l1: 0.0980609\n",
      "[399]\ttraining's l1: 0.0726139\tvalid_1's l1: 0.098054\n",
      "[400]\ttraining's l1: 0.0725984\tvalid_1's l1: 0.0980507\n",
      "[401]\ttraining's l1: 0.0725735\tvalid_1's l1: 0.0980486\n",
      "[402]\ttraining's l1: 0.0725584\tvalid_1's l1: 0.0980511\n",
      "[403]\ttraining's l1: 0.0725451\tvalid_1's l1: 0.0980525\n",
      "[404]\ttraining's l1: 0.0725278\tvalid_1's l1: 0.0980521\n",
      "[405]\ttraining's l1: 0.0725094\tvalid_1's l1: 0.098053\n",
      "[406]\ttraining's l1: 0.0724982\tvalid_1's l1: 0.0980499\n",
      "[407]\ttraining's l1: 0.0724824\tvalid_1's l1: 0.0980355\n",
      "[408]\ttraining's l1: 0.0724654\tvalid_1's l1: 0.0980337\n",
      "[409]\ttraining's l1: 0.0724514\tvalid_1's l1: 0.0980337\n",
      "[410]\ttraining's l1: 0.0724325\tvalid_1's l1: 0.0980375\n",
      "[411]\ttraining's l1: 0.0724141\tvalid_1's l1: 0.0980329\n",
      "[412]\ttraining's l1: 0.0723923\tvalid_1's l1: 0.0980283\n",
      "[413]\ttraining's l1: 0.0723688\tvalid_1's l1: 0.0980328\n",
      "[414]\ttraining's l1: 0.0723497\tvalid_1's l1: 0.0980333\n",
      "[415]\ttraining's l1: 0.0723333\tvalid_1's l1: 0.0980315\n",
      "[416]\ttraining's l1: 0.0723042\tvalid_1's l1: 0.0980408\n",
      "[417]\ttraining's l1: 0.0722896\tvalid_1's l1: 0.0980298\n",
      "[418]\ttraining's l1: 0.0722734\tvalid_1's l1: 0.0980231\n",
      "[419]\ttraining's l1: 0.072253\tvalid_1's l1: 0.0980258\n",
      "[420]\ttraining's l1: 0.0722336\tvalid_1's l1: 0.098021\n",
      "[421]\ttraining's l1: 0.0722161\tvalid_1's l1: 0.0980207\n",
      "[422]\ttraining's l1: 0.0721968\tvalid_1's l1: 0.0980147\n",
      "[423]\ttraining's l1: 0.072179\tvalid_1's l1: 0.0980128\n",
      "[424]\ttraining's l1: 0.0721435\tvalid_1's l1: 0.0980049\n",
      "[425]\ttraining's l1: 0.0721084\tvalid_1's l1: 0.0979929\n",
      "[426]\ttraining's l1: 0.072088\tvalid_1's l1: 0.0979911\n",
      "[427]\ttraining's l1: 0.0720699\tvalid_1's l1: 0.0979886\n",
      "[428]\ttraining's l1: 0.0720535\tvalid_1's l1: 0.0979879\n",
      "[429]\ttraining's l1: 0.0720252\tvalid_1's l1: 0.0979822\n",
      "[430]\ttraining's l1: 0.0719956\tvalid_1's l1: 0.0979749\n",
      "[431]\ttraining's l1: 0.0719626\tvalid_1's l1: 0.0979642\n",
      "[432]\ttraining's l1: 0.0719441\tvalid_1's l1: 0.0979637\n",
      "[433]\ttraining's l1: 0.0719281\tvalid_1's l1: 0.097964\n",
      "[434]\ttraining's l1: 0.0718992\tvalid_1's l1: 0.0979618\n",
      "[435]\ttraining's l1: 0.0718756\tvalid_1's l1: 0.0979571\n",
      "[436]\ttraining's l1: 0.0718418\tvalid_1's l1: 0.0979458\n",
      "[437]\ttraining's l1: 0.0718121\tvalid_1's l1: 0.0979367\n",
      "[438]\ttraining's l1: 0.0717843\tvalid_1's l1: 0.0979349\n",
      "[439]\ttraining's l1: 0.0717507\tvalid_1's l1: 0.0979289\n",
      "[440]\ttraining's l1: 0.0717299\tvalid_1's l1: 0.0979256\n",
      "[441]\ttraining's l1: 0.0716968\tvalid_1's l1: 0.0979166\n",
      "[442]\ttraining's l1: 0.0716769\tvalid_1's l1: 0.0979152\n",
      "[443]\ttraining's l1: 0.0716489\tvalid_1's l1: 0.0979062\n",
      "[444]\ttraining's l1: 0.0716259\tvalid_1's l1: 0.0979124\n",
      "[445]\ttraining's l1: 0.0715978\tvalid_1's l1: 0.0979062\n",
      "[446]\ttraining's l1: 0.0715729\tvalid_1's l1: 0.0979059\n",
      "[447]\ttraining's l1: 0.0715477\tvalid_1's l1: 0.0979054\n",
      "[448]\ttraining's l1: 0.0715192\tvalid_1's l1: 0.0978959\n",
      "[449]\ttraining's l1: 0.0714834\tvalid_1's l1: 0.0978871\n",
      "[450]\ttraining's l1: 0.0714657\tvalid_1's l1: 0.0978832\n",
      "[451]\ttraining's l1: 0.0714425\tvalid_1's l1: 0.0978786\n",
      "[452]\ttraining's l1: 0.0714161\tvalid_1's l1: 0.0978781\n",
      "[453]\ttraining's l1: 0.0713944\tvalid_1's l1: 0.0978783\n",
      "[454]\ttraining's l1: 0.0713598\tvalid_1's l1: 0.0978734\n",
      "[455]\ttraining's l1: 0.0713377\tvalid_1's l1: 0.097863\n",
      "[456]\ttraining's l1: 0.0713108\tvalid_1's l1: 0.097859\n",
      "[457]\ttraining's l1: 0.0712874\tvalid_1's l1: 0.0978452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[458]\ttraining's l1: 0.0712665\tvalid_1's l1: 0.0978448\n",
      "[459]\ttraining's l1: 0.0712428\tvalid_1's l1: 0.0978481\n",
      "[460]\ttraining's l1: 0.0712166\tvalid_1's l1: 0.0978428\n",
      "[461]\ttraining's l1: 0.0711931\tvalid_1's l1: 0.097838\n",
      "[462]\ttraining's l1: 0.0711632\tvalid_1's l1: 0.0978323\n",
      "[463]\ttraining's l1: 0.0711349\tvalid_1's l1: 0.0978298\n",
      "[464]\ttraining's l1: 0.0711102\tvalid_1's l1: 0.0978285\n",
      "[465]\ttraining's l1: 0.0710824\tvalid_1's l1: 0.0978153\n",
      "[466]\ttraining's l1: 0.0710624\tvalid_1's l1: 0.0978141\n",
      "[467]\ttraining's l1: 0.0710462\tvalid_1's l1: 0.097804\n",
      "[468]\ttraining's l1: 0.0710273\tvalid_1's l1: 0.0977983\n",
      "[469]\ttraining's l1: 0.0710061\tvalid_1's l1: 0.0977906\n",
      "[470]\ttraining's l1: 0.0709796\tvalid_1's l1: 0.0977854\n",
      "[471]\ttraining's l1: 0.0709514\tvalid_1's l1: 0.0977849\n",
      "[472]\ttraining's l1: 0.0709292\tvalid_1's l1: 0.0977825\n",
      "[473]\ttraining's l1: 0.070905\tvalid_1's l1: 0.0977847\n",
      "[474]\ttraining's l1: 0.0708807\tvalid_1's l1: 0.0977898\n",
      "[475]\ttraining's l1: 0.0708596\tvalid_1's l1: 0.0977907\n",
      "[476]\ttraining's l1: 0.0708469\tvalid_1's l1: 0.0977886\n",
      "[477]\ttraining's l1: 0.0708263\tvalid_1's l1: 0.0977824\n",
      "[478]\ttraining's l1: 0.0708101\tvalid_1's l1: 0.0977852\n",
      "[479]\ttraining's l1: 0.0707909\tvalid_1's l1: 0.0977838\n",
      "[480]\ttraining's l1: 0.0707601\tvalid_1's l1: 0.0977761\n",
      "[481]\ttraining's l1: 0.0707438\tvalid_1's l1: 0.0977755\n",
      "[482]\ttraining's l1: 0.0707306\tvalid_1's l1: 0.0977738\n",
      "[483]\ttraining's l1: 0.0707141\tvalid_1's l1: 0.0977704\n",
      "[484]\ttraining's l1: 0.0706992\tvalid_1's l1: 0.097775\n",
      "[485]\ttraining's l1: 0.0706828\tvalid_1's l1: 0.0977695\n",
      "[486]\ttraining's l1: 0.0706646\tvalid_1's l1: 0.0977649\n",
      "[487]\ttraining's l1: 0.0706412\tvalid_1's l1: 0.0977626\n",
      "[488]\ttraining's l1: 0.0706229\tvalid_1's l1: 0.0977704\n",
      "[489]\ttraining's l1: 0.0705912\tvalid_1's l1: 0.0977628\n",
      "[490]\ttraining's l1: 0.0705682\tvalid_1's l1: 0.0977576\n",
      "[491]\ttraining's l1: 0.070547\tvalid_1's l1: 0.0977563\n",
      "[492]\ttraining's l1: 0.0705234\tvalid_1's l1: 0.0977489\n",
      "[493]\ttraining's l1: 0.0705042\tvalid_1's l1: 0.0977494\n",
      "[494]\ttraining's l1: 0.0704849\tvalid_1's l1: 0.0977416\n",
      "[495]\ttraining's l1: 0.0704676\tvalid_1's l1: 0.0977426\n",
      "[496]\ttraining's l1: 0.0704515\tvalid_1's l1: 0.0977319\n",
      "[497]\ttraining's l1: 0.0704288\tvalid_1's l1: 0.0977188\n",
      "[498]\ttraining's l1: 0.070409\tvalid_1's l1: 0.0977164\n",
      "[499]\ttraining's l1: 0.0703962\tvalid_1's l1: 0.0977169\n",
      "[500]\ttraining's l1: 0.0703799\tvalid_1's l1: 0.0977119\n",
      "[501]\ttraining's l1: 0.0703647\tvalid_1's l1: 0.0977084\n",
      "[502]\ttraining's l1: 0.0703397\tvalid_1's l1: 0.0977003\n",
      "[503]\ttraining's l1: 0.0703176\tvalid_1's l1: 0.0976964\n",
      "[504]\ttraining's l1: 0.0702942\tvalid_1's l1: 0.0976884\n",
      "[505]\ttraining's l1: 0.0702693\tvalid_1's l1: 0.0976805\n",
      "[506]\ttraining's l1: 0.0702519\tvalid_1's l1: 0.0976735\n",
      "[507]\ttraining's l1: 0.070227\tvalid_1's l1: 0.0976663\n",
      "[508]\ttraining's l1: 0.0701991\tvalid_1's l1: 0.0976608\n",
      "[509]\ttraining's l1: 0.0701798\tvalid_1's l1: 0.0976611\n",
      "[510]\ttraining's l1: 0.0701556\tvalid_1's l1: 0.0976575\n",
      "[511]\ttraining's l1: 0.0701369\tvalid_1's l1: 0.097662\n",
      "[512]\ttraining's l1: 0.0701111\tvalid_1's l1: 0.0976624\n",
      "[513]\ttraining's l1: 0.0700916\tvalid_1's l1: 0.0976575\n",
      "[514]\ttraining's l1: 0.0700522\tvalid_1's l1: 0.0976452\n",
      "[515]\ttraining's l1: 0.0700332\tvalid_1's l1: 0.0976365\n",
      "[516]\ttraining's l1: 0.0700094\tvalid_1's l1: 0.0976393\n",
      "[517]\ttraining's l1: 0.0699885\tvalid_1's l1: 0.0976344\n",
      "[518]\ttraining's l1: 0.0699576\tvalid_1's l1: 0.0976368\n",
      "[519]\ttraining's l1: 0.0699333\tvalid_1's l1: 0.0976275\n",
      "[520]\ttraining's l1: 0.0699196\tvalid_1's l1: 0.0976226\n",
      "[521]\ttraining's l1: 0.0699003\tvalid_1's l1: 0.0976198\n",
      "[522]\ttraining's l1: 0.0698739\tvalid_1's l1: 0.0976178\n",
      "[523]\ttraining's l1: 0.0698547\tvalid_1's l1: 0.0976134\n",
      "[524]\ttraining's l1: 0.0698274\tvalid_1's l1: 0.0976072\n",
      "[525]\ttraining's l1: 0.0698023\tvalid_1's l1: 0.0975986\n",
      "[526]\ttraining's l1: 0.0697875\tvalid_1's l1: 0.0975979\n",
      "[527]\ttraining's l1: 0.0697641\tvalid_1's l1: 0.0975952\n",
      "[528]\ttraining's l1: 0.0697325\tvalid_1's l1: 0.0975989\n",
      "[529]\ttraining's l1: 0.0697149\tvalid_1's l1: 0.0975997\n",
      "[530]\ttraining's l1: 0.0696996\tvalid_1's l1: 0.0976017\n",
      "[531]\ttraining's l1: 0.0696776\tvalid_1's l1: 0.0976007\n",
      "[532]\ttraining's l1: 0.069656\tvalid_1's l1: 0.0975906\n",
      "[533]\ttraining's l1: 0.0696389\tvalid_1's l1: 0.0975889\n",
      "[534]\ttraining's l1: 0.0696208\tvalid_1's l1: 0.0975916\n",
      "[535]\ttraining's l1: 0.0696031\tvalid_1's l1: 0.0975919\n",
      "[536]\ttraining's l1: 0.0695824\tvalid_1's l1: 0.0975765\n",
      "[537]\ttraining's l1: 0.0695651\tvalid_1's l1: 0.0975733\n",
      "[538]\ttraining's l1: 0.0695485\tvalid_1's l1: 0.0975706\n",
      "[539]\ttraining's l1: 0.0695294\tvalid_1's l1: 0.0975663\n",
      "[540]\ttraining's l1: 0.0695081\tvalid_1's l1: 0.0975637\n",
      "[541]\ttraining's l1: 0.0694841\tvalid_1's l1: 0.0975644\n",
      "[542]\ttraining's l1: 0.0694663\tvalid_1's l1: 0.0975635\n",
      "[543]\ttraining's l1: 0.0694471\tvalid_1's l1: 0.0975544\n",
      "[544]\ttraining's l1: 0.069425\tvalid_1's l1: 0.0975603\n",
      "[545]\ttraining's l1: 0.069399\tvalid_1's l1: 0.0975587\n",
      "[546]\ttraining's l1: 0.0693805\tvalid_1's l1: 0.0975555\n",
      "[547]\ttraining's l1: 0.0693638\tvalid_1's l1: 0.0975555\n",
      "[548]\ttraining's l1: 0.0693471\tvalid_1's l1: 0.0975534\n",
      "[549]\ttraining's l1: 0.0693312\tvalid_1's l1: 0.0975506\n",
      "[550]\ttraining's l1: 0.0693112\tvalid_1's l1: 0.0975447\n",
      "[551]\ttraining's l1: 0.0692917\tvalid_1's l1: 0.0975449\n",
      "[552]\ttraining's l1: 0.0692665\tvalid_1's l1: 0.0975426\n",
      "[553]\ttraining's l1: 0.069238\tvalid_1's l1: 0.0975463\n",
      "[554]\ttraining's l1: 0.0692233\tvalid_1's l1: 0.0975438\n",
      "[555]\ttraining's l1: 0.0692091\tvalid_1's l1: 0.0975431\n",
      "[556]\ttraining's l1: 0.0691952\tvalid_1's l1: 0.0975418\n",
      "[557]\ttraining's l1: 0.0691839\tvalid_1's l1: 0.0975407\n",
      "[558]\ttraining's l1: 0.0691619\tvalid_1's l1: 0.0975369\n",
      "[559]\ttraining's l1: 0.0691487\tvalid_1's l1: 0.0975366\n",
      "[560]\ttraining's l1: 0.0691374\tvalid_1's l1: 0.0975289\n",
      "[561]\ttraining's l1: 0.0691287\tvalid_1's l1: 0.0975267\n",
      "[562]\ttraining's l1: 0.0691127\tvalid_1's l1: 0.0975277\n",
      "[563]\ttraining's l1: 0.0690943\tvalid_1's l1: 0.0975278\n",
      "[564]\ttraining's l1: 0.0690809\tvalid_1's l1: 0.0975217\n",
      "[565]\ttraining's l1: 0.0690643\tvalid_1's l1: 0.0975168\n",
      "[566]\ttraining's l1: 0.0690493\tvalid_1's l1: 0.0975162\n",
      "[567]\ttraining's l1: 0.0690389\tvalid_1's l1: 0.0975206\n",
      "[568]\ttraining's l1: 0.0690268\tvalid_1's l1: 0.0975203\n",
      "[569]\ttraining's l1: 0.0690172\tvalid_1's l1: 0.0975201\n",
      "[570]\ttraining's l1: 0.0690086\tvalid_1's l1: 0.0975244\n",
      "[571]\ttraining's l1: 0.0689831\tvalid_1's l1: 0.0975147\n",
      "[572]\ttraining's l1: 0.068975\tvalid_1's l1: 0.0975223\n",
      "[573]\ttraining's l1: 0.0689601\tvalid_1's l1: 0.0975216\n",
      "[574]\ttraining's l1: 0.0689436\tvalid_1's l1: 0.097523\n",
      "[575]\ttraining's l1: 0.0689295\tvalid_1's l1: 0.0975166\n",
      "[576]\ttraining's l1: 0.0689098\tvalid_1's l1: 0.0975178\n",
      "[577]\ttraining's l1: 0.0688913\tvalid_1's l1: 0.0975222\n",
      "[578]\ttraining's l1: 0.0688667\tvalid_1's l1: 0.0975152\n",
      "[579]\ttraining's l1: 0.0688555\tvalid_1's l1: 0.0975131\n",
      "[580]\ttraining's l1: 0.0688406\tvalid_1's l1: 0.0975165\n",
      "[581]\ttraining's l1: 0.0688319\tvalid_1's l1: 0.097511\n",
      "[582]\ttraining's l1: 0.0688231\tvalid_1's l1: 0.0975121\n",
      "[583]\ttraining's l1: 0.0688139\tvalid_1's l1: 0.0975107\n",
      "[584]\ttraining's l1: 0.0688006\tvalid_1's l1: 0.0975109\n",
      "[585]\ttraining's l1: 0.0687915\tvalid_1's l1: 0.0975118\n",
      "[586]\ttraining's l1: 0.0687762\tvalid_1's l1: 0.0975122\n",
      "[587]\ttraining's l1: 0.0687667\tvalid_1's l1: 0.0975107\n",
      "[588]\ttraining's l1: 0.0687585\tvalid_1's l1: 0.0975128\n",
      "[589]\ttraining's l1: 0.0687489\tvalid_1's l1: 0.0975168\n",
      "[590]\ttraining's l1: 0.0687358\tvalid_1's l1: 0.09751\n",
      "[591]\ttraining's l1: 0.0687175\tvalid_1's l1: 0.0975078\n",
      "[592]\ttraining's l1: 0.0687112\tvalid_1's l1: 0.0975083\n",
      "[593]\ttraining's l1: 0.0687049\tvalid_1's l1: 0.0975099\n",
      "[594]\ttraining's l1: 0.0686948\tvalid_1's l1: 0.0975087\n",
      "[595]\ttraining's l1: 0.0686869\tvalid_1's l1: 0.0975091\n",
      "[596]\ttraining's l1: 0.0686798\tvalid_1's l1: 0.0975052\n",
      "[597]\ttraining's l1: 0.0686731\tvalid_1's l1: 0.0975043\n",
      "[598]\ttraining's l1: 0.0686669\tvalid_1's l1: 0.0975086\n",
      "[599]\ttraining's l1: 0.0686616\tvalid_1's l1: 0.0975094\n",
      "[600]\ttraining's l1: 0.0686571\tvalid_1's l1: 0.0975088\n",
      "[601]\ttraining's l1: 0.0686502\tvalid_1's l1: 0.0975091\n",
      "[602]\ttraining's l1: 0.0686399\tvalid_1's l1: 0.0975051\n",
      "[603]\ttraining's l1: 0.0686301\tvalid_1's l1: 0.0975048\n",
      "[604]\ttraining's l1: 0.0686201\tvalid_1's l1: 0.0974999\n",
      "[605]\ttraining's l1: 0.0686132\tvalid_1's l1: 0.0974982\n",
      "[606]\ttraining's l1: 0.068606\tvalid_1's l1: 0.0974923\n",
      "[607]\ttraining's l1: 0.0685987\tvalid_1's l1: 0.0974949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[608]\ttraining's l1: 0.0685892\tvalid_1's l1: 0.0974911\n",
      "[609]\ttraining's l1: 0.068582\tvalid_1's l1: 0.0974889\n",
      "[610]\ttraining's l1: 0.0685711\tvalid_1's l1: 0.0974907\n",
      "[611]\ttraining's l1: 0.0685609\tvalid_1's l1: 0.0974926\n",
      "[612]\ttraining's l1: 0.0685535\tvalid_1's l1: 0.0974963\n",
      "[613]\ttraining's l1: 0.0685471\tvalid_1's l1: 0.0974999\n",
      "[614]\ttraining's l1: 0.0685412\tvalid_1's l1: 0.0975038\n",
      "[615]\ttraining's l1: 0.0685343\tvalid_1's l1: 0.0975042\n",
      "[616]\ttraining's l1: 0.0685284\tvalid_1's l1: 0.0975052\n",
      "[617]\ttraining's l1: 0.0685188\tvalid_1's l1: 0.0974978\n",
      "[618]\ttraining's l1: 0.0685101\tvalid_1's l1: 0.0974979\n",
      "[619]\ttraining's l1: 0.068503\tvalid_1's l1: 0.097497\n",
      "Early stopping, best iteration is:\n",
      "[609]\ttraining's l1: 0.068582\tvalid_1's l1: 0.0974889\n"
     ]
    }
   ],
   "source": [
    "gbm = lgb.train(param,lgb_train,num_boost_round=1500,valid_sets=[lgb_train,lgb_eval],early_stopping_rounds=10)#,callbacks=[lgb.reset_parameter(learning_rate=[0.02]*5000+[0.002]*5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_pred = gbm.predict(x_pred,num_iteration=gbm.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 2000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y1_pred)):\n",
    "    if y1_pred[i]<0:\n",
    "        y1_pred[i] = 0\n",
    "    if y1_pred[i]>1:\n",
    "        y1_pred[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_feature = 'y0.csv'\n",
    "y_file = open(y_feature,\"w\")\n",
    "for i in range(len(y1_pred)):\n",
    "    y_file.write(str(y1_pred[i]))\n",
    "    if i != len(y1_pred)-1:\n",
    "        y_file.write(\"\\n\")\n",
    "y_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_file = open(\"NAE.csv\",\"w\")\n",
    "#with open(\"WMAE.csv\",'r') as f:\n",
    "with open(\"../predict/0610_RNN_NAE.csv\",'r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        print(i)\n",
    "        line_list = line.strip().split(',')\n",
    "        print(line_list)\n",
    "        y_file.write(line_list[0]+\",\"+str(y1_pred[i])+\",\"+line_list[2])\n",
    "        if i!= len(y1_pred)-1:\n",
    "            y_file.write(\"\\n\")\n",
    "y_file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_file = open(\"NAE_2.csv\",\"w\")\n",
    "#with open(\"../predict/0605_RNN_WMAE.csv\",'r') as f:\n",
    "with open(\"../predict/0610_RNN_NAE.csv\",'r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        print(i)\n",
    "        line_list = line.strip().split(',')\n",
    "        print(line_list)\n",
    "        y_file.write(str(y1_pred[i])+\",\"+line_list[1]+\",\"+line_list[2])\n",
    "        if i!= len(y1_pred)-1:\n",
    "            y_file.write(\"\\n\")\n",
    "y_file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
